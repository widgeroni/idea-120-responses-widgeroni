<h1>Bias in the Algorithms</h1>
<br>
<p>
In Weapons of Math Destruction, Cathy O'Neil highlights the inherent flaws in human designed algorithms. To put it simply, anything designed by a human will inherit the conscious and unconscious bias's of its designer. Whether intentional or unintentional, a computer model can only operate on the logic it is programmed with and many times designers are just not able to account for all potential outcomes and behaviors, which creates gaps in human designed systems.</p>
<br>
<p>Computer models often omit intangible data points that are hard to quantify or have no relationship to parallel sets of inputs. Leaving out subjective measurements and focusing purely on objective data creates gaps in the holistic view of a system. These gaps are often where the intangible elements of human designed systems live. As cited in the article, O'Neil points out that evaluating teachers only on measurable and objective information would leave a large body of data unaccounted for. For example, if a teacher was evaluated purely on the performance of their students, than many factors outside of the teachers control would contribute to their performance. Socio-economic factors, income inequality, social pressure, access to proper health care, stable home lives, and more all can impact a students performance but would be unaccounted for in a metrics based evaluation focusing only on academic outcomes. Another major flaw in this example is that teachers were fired without the ability to defend their performance and they were not given an opportunity to address any areas of deficiency as calculated by the evaluation. In this case the draconian outcomes are meant to expedite the chances of improving the overall system by eliminating components it deemed inefficient rather than evaluating and optimizing them. O'Neil points this out, saying the system is designed without a feedback loop to measure whether the decisions made were the correct ones.</p>
<br>
 <p>This creates a scenario in which instead of prioritizing for the best results, people using a system will instead gear their behaviors towards satisfying the requirements of the algorithms. I'm reminded of this <a href="https://www.youtube.com/watch?v=baY3SaIhfl0">humorous video</a> that started making the rounds last year highlighting the disconnect between intended behaviors and behaviors that also achieved the same results. In this scenario the intended result was that the blocks would wind up inside the container through holes that matched the shape of the object. But how the blocks got into the container was up to the user. We see the designer growing increasingly frustrated as the user puts shapes that were intended to go in different holes in the same hole because they all fit. I think this perfectly illustrates the idea that even when we attempt to design a system that optimizes for certain behaviors, we often overlook some of the subjective decisions that are made when interacting with a system. A computer, attempting to take the most optimal path, would make some of the same decision the user did. Instead of having to evaluate the shape of an object in relation to the shape of the holes, it might look at it's volumetric proportions and decide that all of the shapes could fit in one hole, which would be the most efficient path.</p>
<br>
<p> Philip K Dick, in his 1956 novella (and subsequent film adaptation) "The Minority Report" warned of using predictive models on human behavior. In the book, "pre-cogs" are able to predict crimes prior to their occurrence, but throughout the plot it is revealed that often times within the system there were conflicting interpretations on the events that were yet to occur. The underlying message of the book is that even with seemingly congruent data, evaluation of the data and the selection of an outcome is riddled with opportunities for misinterpretation.</p>
<br>
So the idea that algorithms and predictive models, whether in science fiction or practice, have flaws is nothing new. But the current fascination with optimizing every aspect of society to eliminate bias and human error has created its own set of problems. It may seem altruistic to attempt to eliminate prejudice and bias as society becomes more globally connected and diverse, but there are massive risks to consider when turning decision making over to a human designed system that is limited to data sets that cannot account for all possible factors. There is a middle ground to be had, but society must recognize and accept its own flaws before we build them into an algorithm.
